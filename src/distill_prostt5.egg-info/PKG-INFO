Metadata-Version: 2.4
Name: distill_prostt5
Version: 0.5.0
Summary: Distillation Commands for ProstT5
Author-email: George Bouras <george.bouras@adelaide.edu.au>
Project-URL: Homepage, https://github.com/gbouras13/distill_prostt5
Keywords: keyword,are,cool
Classifier: Development Status :: 4 - Beta
Classifier: Environment :: Console
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Classifier: Topic :: Scientific/Engineering :: Bio-Informatics
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: <3.13,>=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: click>=8.0.0
Requires-Dist: loguru>=0.5.3
Requires-Dist: pyyaml>=6.0
Requires-Dist: pandas>=1.4.2
Requires-Dist: biopython>=1.80
Requires-Dist: datasets>=2.15
Requires-Dist: requests>=2.25
Requires-Dist: sentencepiece>=0.1.99
Requires-Dist: transformers>=4.34
Requires-Dist: torch>=2.1.2
Requires-Dist: pyarrow>=14.0.0
Requires-Dist: alive-progress>=3.0.1
Requires-Dist: numpy<2,>=1.20
Requires-Dist: h5py>=3.5
Requires-Dist: accelerate>=0.26.0
Requires-Dist: tqdm>=4.30.0
Provides-Extra: lint
Requires-Dist: isort; extra == "lint"
Requires-Dist: black; extra == "lint"
Provides-Extra: test
Requires-Dist: pytest>=6.2.5; extra == "test"
Requires-Dist: pytest-cov>=3.0.0; extra == "test"
Provides-Extra: exe
Requires-Dist: setuptools; extra == "exe"
Requires-Dist: wheel; extra == "exe"
Requires-Dist: build; extra == "exe"
Dynamic: license-file

# distill_prostt5
Distillation Commands for ProstT5

# Install

```
git clone https://github.com/gbouras13/distill_prostt5
cd distill_prostt5
pip install -e . 
```

# Prod - Proceed with no Logits and `-a 1`

## Step 1 - tokenise input data

* This takes an amino acid protein FASTA file with corresponding 3Di FASTA as input, and will write out a `.h5` file with tokenized input

e.g.

```bash
distill_prostt5 precompute --no_logits -i tests/test_data/phrog_3922_db_aa.fasta -c tests/test_data/phrog_3922_db_ss.fasta  -p test.hdf5 -m 512
distill_prostt5 precompute --no_logits -i tests/test_data/swissprot_subset_aa_500.fasta -c tests/test_data/swissprot_subset_ss_500.fasta -p swissprot_subset_aa_500.h5

```

* Actual command

```bash
python scripts/filter_fastas.py -c 10000clusters.fasta -i fasta/prostT5_dataset.fasta -o prostT5_filt_aa.fasta
python scripts/filter_fastas.py -c 10000clusters.fasta -i fasta/prostT5_dataset_ss.fasta  -o prostT5_filt_ss.fasta

distill_prostt5 precompute --no_logits -i prostT5_filt_aa.fasta -c prostT5_filt_ss.fasta -p prostT5_training.h5
distill_prostt5 precompute --no_logits -i 10000clusters.fasta -c 10000clusters_ss.fasta -p prostT5_validation.h5
```

## Step 2 - train 

* Trains mini ProstT5 distilled model
* USE `-a 1 --no_logits` to enforce only CE loss 
* Uses the ModernBertModel https://huggingface.co/docs/transformers/en/model_doc/modernbert#transformers.ModernBertModel
* 11M params by default - you can see the exact architecutre in `distill_prostt5/classes/MPROSTT5_bert.py`
* It is vanilla with 6 layers, 8 attention heads, and hidden dimension of 512

```bash
 distill_prostt5 train -p test.hdf5 -e swissprot_subset_aa_500.h5  -o test_out_500_nl --no_logits  -a 1

  distill_prostt5 train -p prostT5_training.h5 -e prostT5_validation.h5  -o test_out --no_logits  -a 1
  distill_prostt5 train -p prostT5_validation.h5 -e  swissprot_subset_aa_500.h5  -o test_out --no_logits  -a 1 --epochs 2 -b 4 --logging_eval_steps 1000
  

```

### Singletons

```bash
distill_prostt5 precompute --no_logits -i prostt5_dataset_singletons.fasta -c prostt5_dataset_singletons_ss.fasta -p prostT5_singletons.h5
```





# Testing (Logits vs no Logits)

## Step 1 - precompute ProstT5 embeddings

* This takes an amino acid protein FASTA file with corresponding 3Di FASTA as input, and will write out a `.h5` file with the ProstT5-CNN logits and the tokenized input

e.g.

```bash
distill_prostt5 precompute -i tests/test_data/phrog_3922_db_aa.fasta -c tests/test_data/phrog_3922_db_ss.fasta -p phrog_3922_db_aa.h5
distill_prostt5 precompute -i tests/test_data/swissprot_subset_aa_500.fasta -c tests/test_data/swissprot_subset_ss_500.fasta -p swissprot_subset_aa_500.h5
```

## Step 2 - merge ProstT5 embeddings

* This takes a directory of `.h5` files made in step 1 as input and will write out a single `.h5` file (for training)
* Idea is to batch on e.g. Pawsey for preparing a full training dataset
* ALSO v0.2.0 - changes the format from 1 group per protein (very very inefficient for 17M dataset) to 4 datasets (input_ids, labels, attention_mask and targets) with 17M arrays
* Would also change `precompute` but have computed the dataset already which is the most compute hungry part, so therefore need to run the merge functionality


```bash
distill_prostt5 merge -d tests/test_h5s/ -p merged.h5
```


## Step 3 - train 

* Trains mini ProstT5 distilled model
* Uses the ModernBertModel https://huggingface.co/docs/transformers/en/model_doc/modernbert#transformers.ModernBertModel
* 11M params - you can see the exact architecutre in `distill_prostt5/classes/MPROSTT5_bert.py`
* It is vanilla with 6 layers, 8 attention heads, and hidden dimension of 512
* Will ablate this
* The training loss is `loss = (1-alpha)* kl_loss + alpha * ce_loss` (`alpha` is 0.3 for now, will ablate)
    * `kl_loss` is vs the ProstT5-CNN logits 
    * `ce_loss` is vs the colabfold 3Di "ground truth"



```bash
distill_prostt5 train -p swissprot_subset_aa_500.h5 -e phrog_3922_db_aa.h5  -o test_out_500 
```

* To train using DDP

```
torchrun \
    --nproc_per_node=$NUM_GPUS_PER_NODE \
    --nnodes=$SLURM_JOB_NUM_NODES \
    --rdzv_id=$SLURM_JOB_ID \
    --rdzv_backend=c10d \
    --rdzv_endpoint="$RDZV_HOST:$RDZV_PORT" \
    distill_prostt5/run.py train ...
```

```bash
Usage: distill_prostt5 train [OPTIONS]

  Trains distilled Mini ProstT5 model

Options:
  -h, --help                    Show this message and exit.
  -p, --train_path PATH         Path to .h5 file containing training data
                                processed with the precompute subcommand
                                [required]
  -e, --eval_path PATH          Path to .h5 file containing evaluation data
                                processed with the precompute subcommand
                                [required]
  -o, --output_dir PATH         Output directory where checkpoints will be
                                saved   [required]
  -m, --model_ckpt PATH         Model checkpoint directory (to restart
                                training from here)
  -b, --batch_size INTEGER      Batch size per device - 192 can fit in MI250
                                GPU memory
  --epochs INTEGER              Epochs
  -a, --alpha FLOAT             Weighted contribution of Colabfold CE loss to
                                total loss
  --activation TEXT             activation type - choose gelu or swiglu,
                                defaults to swiglu
  --num_layers INTEGER          Number of layers (default to 6)
  --num_heads INTEGER           Number of attention heads (default to 8)
  --hidden_size INTEGER         Hidden size (default to 512)
  --learning_rate FLOAT         learning rate (default to 3e-4)
  --save_steps INTEGER          Save checkpoint this many steps (default to
                                1000)
  --logging_eval_steps INTEGER  Eval and log at this many steps (default to
                                25)
  ```

 ## Step 4 - infer

 ```bash
 distill_prostt5 infer -i tests/test_data/swissprot_subset_aa_50.fasta -o test_infer -m checkpoint-308000/
 ```

 ```bash
 Usage: distill_prostt5 infer [OPTIONS]

  Infers 3Di from input AA FASTA

Options:
  -h, --help             Show this message and exit.
  -i, --input PATH       Input FASTA  [required]
  -o, --output_dir PATH  Output directory  [required]
  -m, --model_ckpt PATH  Model checkpoint directory (to predict 3Di using
                         this)
  --cpu                  Use cpus only.
  ```


  # 25 August 2025.  


* Updating the container (for focal loss) degraded performance on the original training task
* Namely, training a 110M param model, I couldn't get the training loss to go below 2.5 (whereas on the old container it was fine)

```bash
singularity exec distill_prostt5_0.4.1.sif python -c "
> import torch
> print('PyTorch version:', torch.__version__)
> print('CUDA version:', torch.version.cuda)
> print('ROCm version:', torch.version.hip if hasattr(torch.version, 'hip') else 'N/A')
> print('Is CUDA available?', torch.cuda.is_available())
> print('Is ROCm available?', torch.version.hip is not None)
> print('Available devices:', [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])
> "
PyTorch version: 2.8.0.dev20250517+rocm6.3
CUDA version: None
ROCm version: 6.3.42131-fa1d09cbd
Is CUDA available? False
Is ROCm available? True
Available devices: []

singularity exec distill_prostt5_0.5.0.sif python -c "
> import torch
> print('PyTorch version:', torch.__version__)
> print('CUDA version:', torch.version.cuda)
> print('ROCm version:', torch.version.hip if hasattr(torch.version, 'hip') else 'N/A')
> print('Is CUDA available?', torch.cuda.is_available())
> print('Is ROCm available?', torch.version.hip is not None)
> print('Available devices:', [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])
> "

PyTorch version: 2.9.0.dev20250822+rocm6.3
CUDA version: None
ROCm version: 6.3.42134-a9a80e791
Is CUDA available? False
Is ROCm available? True


python distill_prostt5/run.py train -p /mynvme/data/prostT5_training.h5 -e  /mynvme/data/prostT5_validation.h5  -o $OUTDIR --learning_rate 8e-4  --no_logits --warmup_ratio $WARMUP_RATIO  -a 1 --epochs 12 -b $BATCH_SIZE --logging_eval_steps 1000  --num_workers 8 --num_heads $NUM_HEADS --num_layers $NUM_LAYERS --hidden_size $HIDDEN_DIM --intermediate_size $INTERMEDIATE_DIM --save_steps 1000

```

curl -OL https://download.pytorch.org/whl/nightly/cpu/torch-2.1.0.dev20230602%2Bcpu-cp310-cp310-linux_x86_64.whl
